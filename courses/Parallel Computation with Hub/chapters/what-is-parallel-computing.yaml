- Category: text
  Prompt: "In this tutorial we will be using the animals dataset that is shipped with this repo.\nPlease have it in your working directory."

- Category: text
  Prompt: "When we execute a function in Python, it usually runs on a single thread.\nMost modern processors can handle multiple threads at the same time.\nSo, hub uses multiple threads to effectively distribute the workload amongst multiple threads to speed up the entire process.\nThis is called parallel computing."

- Category: text
  Prompt: "Hub enables you to easily run computations in parallel, which significantly accelerates workflows\nsuch as uploading datasets, making changes to datasets, running inference on your datasets, and many others."

- Category: text
  Prompt: "In order to utilize this feature we must learn about 'hub.compute'.\nIn this tutorial we will focus on uploading datasets faster using parallel computing."

- Category: mcq
  Prompt: "`hub.compute` is a function decorator, what does that mean?"
  Options:
    - It is placed right above the function definition, and it modifies the behavior of the function.
    - It makes the code look pretty
    - It exists to improve the human readability of code.
  Answer: It is placed right above the function definition, and it modifies the behavior of the function.
  Hints:
    - That's correct!
    - It might, but that's not mainly why it is used.
    - It might, but that's not why it is used.

- Category: text
  Prompt: "So since, hub.compute is a decorator,\nwe must first create a function which it will wrap around."

- Category: code
  Prompt: "Start by importing hub. Type `import hub` to do so."
  Answer: import hub
  Hint: Do you have Hub installed?

- Category: code
  Prompt: "We must also import os for this tutorial. Type `import os` to do so."
  Answer: import os
  Hint: Check the syntax for errors.

- Category: code
  Prompt: "We will also need numpy. Type `import numpy as np`"
  Answer: import numpy as np
  Hint: Do you have numpy installed?

- Category: text
  Prompt: "Now, let's create a function to populate a dataset."

- Category: code
  Prompt: |
    Type the following function:

    @hub.compute
    def file_to_hub(file_name, sample_out, class_names):
        label_text = os.path.basename(os.path.dirname(file_name))
        label_num = class_names.index(label_text)
        sample_out.labels.append(np.uint32(label_num))
        sample_out.images.append(hub.read(file_name))
        return sample_out
  Answer: |
    @hub.compute
    def file_to_hub(file_name, sample_out, class_names):
        label_text = os.path.basename(os.path.dirname(file_name))
        label_num = class_names.index(label_text)
        sample_out.labels.append(np.uint32(label_num))
        sample_out.images.append(hub.read(file_name))
        return sample_out
  Hint: Check the syntax for errors.

- Category: text
  Prompt: "Nice!  Let's see what the  three parameters in the function do.\nThe first two are default and  other parameters are optional."

- Category: text
  Prompt: "The first argument must be an element of the variable we will be iterating through.\nIn this case, that is a filename file_name, because file_to_hub reads image files and populates data in the dataset's tensors."

- Category: text
  Prompt: "The second argument is a dataset sample sample_out,\nwhich can be operated on using similar syntax to dataset objects, such as sample_out.append(...), sample_out.extend(...), etc."

- Category: text
  Prompt: "The function decorated using @hub.compute must return sample_out,\nwhich represents the data that is added or modified by that function."

- Category: code
  Prompt: |
    To execute the transform, you must define the dataset that will be modified by the parallel computation. So let us do that by typing:

    ds = hub.empty('./animals_hub_transform')
  Answer: ds = hub.empty('./animals_hub_transform')
  Hint: Check the syntax for errors.

- Category: code
  Prompt: |
    Now we need a list of files that we will pass into our transform, for that we first need to locate our dataset. 
    Type `dataset_folder = './animals'` 
    and please keep the animals dataset in the working directory, otherwise the following code snippets won’t work.
  Answer: dataset_folder = './animals'
  Hint: Check the syntax for errors.

- Category: code
  Prompt: |
    We can also fetch the names of the classes to be passed as our 3rd argument
    Run `class_names = os.listdir(dataset_folder)`
  Answer: class_names = os.listdir(dataset_folder)
  Hint: Check the syntax for errors.

- Category: code
  Prompt: |
    Let us create an empty list of files that we will populate later.
    Type `files_list = []`
  Answer: files_list = []
  Hint: Check the syntax for errors.

- Category: code
  Prompt: |
    Now we populate the list with the following loop:

    for dirpath, dirnames, filenames in os.walk(dataset_folder):
        for filename in filenames:
            files_list.append(os.path.join(dirpath, filename))
  Answer: |
    for dirpath, dirnames, filenames in os.walk(dataset_folder):
          for filename in filenames:
              files_list.append(os.path.join(dirpath, filename))
  Hint: Check the syntax for errors.

- Category: text
  Prompt: "Now everything is ready for us to run the parallel computation using the `.eval()` syntax.\nWe shall pass the optional input arguments to file_to_hub and skip the first two default arguments file_name and sample_out."

- Category: text
  Prompt: "The input iterable files_list and output dataset ds is passed to the `.eval()` method\nas the first and second argument respectively."

- Category: code
  Prompt: |
    Type the following snippet to populate the dataset using parallel computation:

    with ds:
        ds.create_tensor('images', htype = 'image', sample_compression = 'jpeg')
        ds.create_tensor('labels', htype = 'class_label', class_names = class_names)
        file_to_hub(class_names=class_names).eval(files_list, ds, num_workers = 2)
  Answer: |
    with ds:
        ds.create_tensor('images', htype = 'image', sample_compression = 'jpeg')
        ds.create_tensor('labels', htype = 'class_label', class_names = class_names)
        file_to_hub(class_names=class_names).eval(files_list, ds, num_workers = 2)
  Hint: Check the syntax for errors.

- Category: text
  Prompt: "The num_workers argument determines how many parallel threads the program will be run on."

- Category: code
  Prompt: |
    Let us fetch a label from the dataset to check if the upload worked.
    Type `ds.labels[0].numpy()` 
    If this returns a proper value then we can be assured that the upload worked properly.
  Answer: ds.labels[0].numpy()
  Hint: Check the syntax for errors.

- Category: text
  Prompt: And it did! But before we end, let's try to answer a question.

- Category: mcq
  Prompt: "Every compute function has two default arguments, how do we pass them while executing the function?"
  Options:
    - Normally like every other function.
    - We don't pass them, the code automatically infers which values to take.
    - We skip them in the main function call but pass them in the eval method.
  Answer: We skip them in the main function call but pass them in the eval method.
  Hints:
    - We actually pass them to the eval function :)
    - We actually pass them to the eval function :)
    - That's correct!

- Category: code
  Prompt: |
    Let us free up memory by deleting the dataset
    Type `ds.delete()`
  Answer: ds.delete()
  Hint: Check the syntax for errors.

- Category: text
  Prompt: "That's it for this tutorial.\nNow you know how to use Hub’s parallel computation feature to upload datasets faster."
